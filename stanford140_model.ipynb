{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a classifier on sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#from: http://help.sentiment140.com/for-students/\n",
    "\n",
    "'''\n",
    "0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "1 - the id of the tweet (2087)\n",
    "2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "4 - the user that tweeted (robotickilldozr)\n",
    "5 - the text of the tweet (Lyx is cool)\n",
    "'''\n",
    "\n",
    "s140_train = pd.read_csv('model_data/training.1600000.processed.noemoticon.csv', encoding='latin-1',\n",
    "                         names=['sentiment','id','date','query','user','text'], header = None)\n",
    "\n",
    "s140_train = s140_train[['sentiment','text']]\n",
    "\n",
    "s140_train = s140_train[s140_train['sentiment'] != 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = s140_train['text']\n",
    "y_train = np.where(s140_train['sentiment'] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1600000,), (1600000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1     is upset that he can't update his Facebook by ...\n",
       "2     @Kenichan I dived many times for the ball. Man...\n",
       "3       my whole body feels itchy and like its on fire \n",
       "4     @nationwideclass no, it's not behaving at all....\n",
       "5                         @Kwesidei not the whole crew \n",
       "6                                           Need a hug \n",
       "7     @LOLTrish hey  long time no see! Yes.. Rains a...\n",
       "8                  @Tatiana_K nope they didn't have it \n",
       "9                             @twittera que me muera ? \n",
       "10          spring break in plain city... it's snowing \n",
       "11                           I just re-pierced my ears \n",
       "12    @caregiving I couldn't bear to watch it.  And ...\n",
       "13    @octolinz16 It it counts, idk why I did either...\n",
       "14    @smarrison i would've been the first, but i di...\n",
       "15    @iamjazzyfizzle I wish I got to watch it with ...\n",
       "16    Hollis' death scene will hurt me severely to w...\n",
       "17                                 about to file taxes \n",
       "18    @LettyA ahh ive always wanted to see rent  lov...\n",
       "19    @FakerPattyPattz Oh dear. Were you drinking ou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_nltk = set(stopwords.words(\"english\"))\n",
    "relevant_words = set(['not', 'nor', 'no', 'wasn', 'ain', 'aren', 'very', 'only', 'but', 'don', 'isn', 'weren'])\n",
    "stopwords_filtered = list(stopwords_nltk.difference(relevant_words))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                                    stop_words = stopwords_filtered, max_features = 10000, ngram_range = (1,2))\n",
    "\n",
    "words_matrix = vectorizer.fit_transform(X_train)\n",
    "vocabulary = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '02',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '10 30',\n",
       " '10 days',\n",
       " '10 hours',\n",
       " '10 mins',\n",
       " '10 minutes',\n",
       " '10 years',\n",
       " '100',\n",
       " '100 followers',\n",
       " '1000',\n",
       " '100th']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          coef             word\n",
      "6048  3.595189       no problem\n",
      "6167  3.377626          not sad\n",
      "6065  3.253633       no worries\n",
      "6205  3.252938    nothing wrong\n",
      "4413  3.216784          isn bad\n",
      "7395  3.000354          sad sad\n",
      "6047  2.944481          no prob\n",
      "9422  2.921266         wasn bad\n",
      "6105  2.845657          not bad\n",
      "2296  2.795189         don miss\n",
      "6049  2.655038         no probs\n",
      "9641  2.588553        wish luck\n",
      "6101  2.296067        not alone\n",
      "7841  2.294130          smiling\n",
      "6039  2.267053          no need\n",
      "1740  2.216351  congratulations\n",
      "1390  2.203413      cannot wait\n",
      "1404  2.065235        cant wait\n",
      "9507  2.006067          welcome\n",
      "6020  1.962237         no doubt\n",
      "6054  1.959802        no school\n",
      "5282  1.933175         made day\n",
      "3837  1.915842        hate hate\n",
      "3332  1.911487          go girl\n",
      "6732  1.901858         pleasure\n",
      "4054  1.879394          honored\n",
      "8518  1.869588            thank\n",
      "2281  1.864983       don forget\n",
      "9874  1.859214            yayyy\n",
      "5742  1.834959      musicmonday\n",
      "...        ...              ...\n",
      "1022 -2.524377           booooo\n",
      "2137 -2.538526        depressed\n",
      "7291 -2.555921              rip\n",
      "3554 -2.566748      got excited\n",
      "2138 -2.600060       depressing\n",
      "2215 -2.628301     disappointed\n",
      "7357 -2.653243           ruined\n",
      "6118 -2.665916      not excited\n",
      "1088 -2.671449     breaks heart\n",
      "2165 -2.744418       devastated\n",
      "6146 -2.756188       not liking\n",
      "7400 -2.757842            sadly\n",
      "3716 -2.804847           gutted\n",
      "2216 -2.845280    disappointing\n",
      "9183 -2.862736        upsetting\n",
      "7398 -2.872872         saddened\n",
      "7237 -2.879552       rest peace\n",
      "6149 -2.888978      not looking\n",
      "3896 -2.889476    heartbreaking\n",
      "1166 -2.892228           bummed\n",
      "1729 -2.939093      condolences\n",
      "7399 -2.952320          saddest\n",
      "3897 -3.001210      heartbroken\n",
      "3802 -3.059028    happy fathers\n",
      "1013 -3.059768           boohoo\n",
      "7379 -3.152986              sad\n",
      "5174 -3.324029      lost please\n",
      "6523 -3.369222      passed away\n",
      "6137 -3.568703        not happy\n",
      "4329 -3.990139  inaperfectworld\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_model = LogisticRegression() \n",
    "logistic_model.fit(words_matrix, y_train)\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "coefs = logistic_model.coef_\n",
    "word_importances = pd.DataFrame({'word': vocabulary, 'coef': coefs.tolist()[0]})\n",
    "word_importances_sorted = word_importances.sort_values(by='coef', ascending = False)\n",
    "print(word_importances_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectorizer.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(logistic_model, 'logistic_model.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(vectorizer, 'vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p35]",
   "language": "python",
   "name": "conda-env-p35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
